<!DOCTYPE html>
<html lang="en">

<head>
	<!-- Required meta tags -->
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
	<!-- Bootstrap core CSS -->
	<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/css/bootstrap.min.css" integrity="sha384-/Y6pD6FV/Vv2HJnA6t+vslU6fwYXjCFtcEpHbNJ0lyAFsXTsjBbfaDjzALeQsN6M" crossorigin="anonymous">
	<!-- https://fontawesome.com/cheatsheet -->
	<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css" integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons/css/academicons.min.css">
	<!-- Custom styles for this template -->
	<link rel="stylesheet" href="css/style.css">
    <script src="js/main.js"></script>
    <script src="js/scroll.js"></script>
    <meta name="keywords" content="Boubacar Diallo, Deep Learning, Computer Vision, Machine, Learning, Artificial Intelligence"> 
	<meta name="description" content="Personal page of Boubacar Diallo">
</head>

<title>Boubacar DIALLO</title>

<body>
	<nav class="navbar navbar-expand-md navbar-dark fixed-top bg-dark" id="Home">
		<div class="container">
		<a class="navbar-brand" href="index.html">Boubacar DIALLO</a>
		<div class="collapse navbar-collapse" id="navbarToggle">
			<ul class="navbar-nav ml-auto">
				<li class="nav-item">
					<a class="nav-link" href="index.html">Home</a>
				</li>
				<!-- <li class="nav-item">
					<a class="nav-link" href="lab.html">Lab</a>
				</li> -->
				<li class="nav-item">
					<a class="nav-link" href="publication.html">Publication</a>
				</li>
				<!-- <li class="nav-item">
					<a class="nav-link" href="award.html">Award</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="service.html">Service</a>
				</li> -->
				<li class="nav-item">
					<a class="nav-link" href="talk.html">Tutorials</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="more.html">Curriculum Vitae</a>
				</li>
			</ul>
		</div>
		</div>
	</nav>

	<div class="container" style="padding-top: 20px; font-size: 17px">
		<div class="row">
			<div class="col-md-3", style="padding-right: 0px; padding-top: 0px">
				<img class="img-responsive img-rounded" src="picture/boubacardiallo2.png" alt="" style="max-width: 220px; solid black"><br>
			</div>
			<div class="col-md-5", style="padding-right: 40px; padding-top: 0px">
				<h2>Boubacar DIALLO Ph.D</h2>
				<p>
					<b><font color="black">Senior AI-Vision R&D Engineer at Exxact Robotics</font></b><br>
					<b><font color="black">Ph.D. in AI and CV from Poitiers University</font></b><br>
					<b><font color="black">CEO of IMPACT AI</font></b><br>
					Address: 1 Rue Vincent Ballu<br>
					51200 √âpernay, France<br>
					Email: <a href="boubacar.diallo.perso[at]gmail.com">boubacar.diallo.perso@gmail.com</a><br><br>

					<a href="https://scholar.google.com/citations?user=u2yppEsAAAAJ&hl" target="_blank"><font color="black"><i class="ai ai-google-scholar ai-lg"></i></font></a>&nbsp;&nbsp;&nbsp;/&nbsp;&nbsp;
					<a href="https://github.com/boubacardiallo20" target="_blank"><font color="black"><i class="fab fa-github fa-lg"></i></font></a>&nbsp;&nbsp;&nbsp;/&nbsp;&nbsp;
					<a href="https://x.com/DialloBoubaAI" target="_blank"><font color="black"><i class="fab fa-twitter fa-lg"></i></font></a> &nbsp;&nbsp;&nbsp;/&nbsp;&nbsp;
					<a href="https://sfa.univ-poitiers.fr/" target="_blank">
						<img src="picture/logo/poitiers.png" alt="Precision Agriculture" class="img-fluid" style="width: 10%;">
					</a>&nbsp;&nbsp;&nbsp;/&nbsp;&nbsp;
					<a href="https://www.xlim.fr/" target="_blank">
						<img src="picture/logo/xlim.png" alt="Generative Modeling" class="img-fluid" style="width: 10%;">
					</a>&nbsp;&nbsp;&nbsp;/&nbsp;&nbsp;
					<a href="https://exxact-robotics.com/en/" target="_blank">
						<img src="picture/logo/exr.png" alt="Autonomous Systems" class="img-fluid" style="width: 15%;">
					</a>
				</p>
			</div>
			<div class="col-md-4", style="padding-right: 40px; padding-top: 15px">
                <font color="black"><h5><b>Research Interests:</b></h5>
					<ul>
						<li><a><i>AI - Computer Vision</i></a></li>
						<li><a><i>Visual scene Understanding</i></a></li>
						<li><a><i>LLM and Multimodal Learning</i></a></li>
						<li><a><i>Perception / Autonomous Driving</i></a></li>
						<li><a><i>Robot Learning and Embodied AI</i></a></li>
						<li><a><i>Precision Agriculture and Robotics</i></a></li>
						<li><a><i>Generative Modeling (synthetic data)</i></a></li>
					</ul>
				</font>
			</div>
		</div>
		
		<div class="row">
			<div class="col-md-12">
			<p>
				<div style="text-align:justify">I was a Senior AI R&D Engineer in the AI/Vision Team at 
					<a href="https://exxact-robotics.com/en/">Exxact Robotics</a>, 
					where we focused on innovative solutions for precision agriculture. 
					My current research interests and directions were centered on advancing AI to address real-world challenges, 
					particularly in the domains of precision agriculture, robotics, and autonomous systems. They included: </div>
			</p>
			<p>
				<div style="text-align:justify">
					<li><a><i>Creation and Deployment of Robust AI Models for Precision Agriculture and Robotics.</i></a></li> 
					<li><a><i>Visual scene understanding, perception, reconstruction, representation learning, multimodal learning.</i></a></li> 
					<li><a><i>Generative modeling, creation, generation and manipulation of visual content (synthetic images/3D).</i></a></li> 
					<li><a><i>Autonomous driving, Embodied AI, Robotic Learning, RL and LLM applications, etc...</i></a></li>
			</p>
			<p>
				<div style="text-align:justify">
					Previously, I had the pleasure of working as a Doctoral Researcher (Ph.D.) in the ICONES team of the 
					<a href="https://www.xlim.fr/recherche/pole-mathematiques-informatique-image/synthese-analyse-dimages">ASALI Axis</a> 
					within <a href="https://www.xlim.fr/">XLIM UMR CNRS</a>, - Multidisciplinary Research Institute, 
					<a href="https://sfa.univ-poitiers.fr/">University of Poitiers</a>. 
					My research focused on <a href="https://theses.fr/2020POIT2293"><i><u>Image Integrity Measurement: 
						From Physical Models to Deep Learning Models</u></i></a>, under the supervision of 
						<a href="https://xlim-sic.labo.univ-poitiers.fr/maloigne/">Prof. Christine Fernandez</a>, 
						<a href="https://xlim-sic.labo.univ-poitiers.fr/turruty/">Assoc. Prof. Thierry Urruty</a>, 
						and <a href="https://xlim-sic.labo.univ-poitiers.fr/pbourdon/">Assoc. Prof. Pascal Bourdon</a>. 
				</div>
			</p>
			<p>
				<div style="text-align:justify">
					Before starting my Ph.D., I gained valuable experience as a Research Intern at the XLIM laboratory, 
					where I collaborated with <a href="https://xlim-sic.labo.univ-poitiers.fr/perrine/">Assoc. Prof. Clency Perrine</a> 
					on the project <i><u>Implementation of a Software-Defined Radio Platform: Application to PAPR Reduction</u></i>.
				</div>
			</p>
			<div style="text-align:justify; padding: 10px; border: 1px solid #e9cbcb; border-radius: 5px; background-color: #f9f9f9;">
			<p>
				<div style="text-align:justify">üíûÔ∏è I am passionate about collaborating on projects that have a positive impact on humanity and the planet. 
					With over ten publications covering topics such as deep learning, digital image tampering detection, artificial intelligence applied 
					to precision agriculture, and image processing, I strive to contribute meaningfully to these evolving field.</div>
			</p>
			<p>
				<div style="text-align:justify">üìñ <b>Educator:</b> In addition to my research activities, I am actively engaged in training the next 
					generation of innovators. I design and publish online courses and offer tutorials to disseminate the latest advances and applications 
					in my field, thus helping to make knowledge accessible and inspire future talents.</div>
			</p>
			</div>
			</div>
		</div>
	</div>

  <!-- News -->
	<!-- <div class="container" style="padding-top: 20px; font-size: 17px">
		<h3 id="News" style="padding-top: 80px; margin-top: -80px; margin-left: -18px;">News</h3>
		<ul>
			<li>Our <a href="https://depth-anything-v2.github.io">Depth Anything</a> is integrated into <a href="https://developer.apple.com/machine-learning/models">Apple Core ML Models</a> for fantastic applications.</li>
			<li>Our <a href="https://depth-anything.github.io">Depth Anything</a> won the CVPR 2024 <a href="https://cvpr.thecvf.com/Conferences/2024/News/Wrap_Release">Best Demo Honorable Mention</a>.</li>
			<li>Our <a href="https://arxiv.org/abs/2312.10035">Point Transformer V3</a> won the CVPR 2024 <a href="https://waymo.com/open/challenges">Waymo 3D Semantic Segmentation Challenge Champion</a>.</li>
			<li>Invited talk at the CVPR 2024 <a href="https://sites.google.com/view/cvpr24-ac-workshop">Area Chair Workshop</a>.</li>
			<li>Invited talk at the ECCV 2024 Workshop on <a href="https://syntheticdata4cv.wordpress.com">Synthetic Data for Computer Vision</a>.</li>
			<li>Invited talk at the ECCV 2024 Workshop on <a href="https://lsvos.github.io">Large-scale Video Object Segmentation</a>.</li>
			<li>We are organizing the CVPR 2024 Tutorial on <a href="https://sites.google.com/view/pcn-cvpr24tutorial/homepage">All You Need To Know About Point Cloud Understanding</a>.</li>
			<li>We are organizing the ICML 2024 Workshop on <a href="icml-mfm-eai.github.io">Multimodal Foundation Models for Embodied Agents</a>.</li>
			<li>I am recognized as one of the most influential scholars in computer vision by AI 2000 in <a href="https://www.aminer.org/profile/hengshuang-zhao/542c02afdabfae216e61f9c4">2022, 2023, and 2024</a>.</li>
			<li>I serve as an Area Chair for CVPR 2023, NeurIPS 2023, WACV 2023, CVPR 2024, ECCV 2024, NeurIPS 2024, ACMMM 2024.</li>
			<li>I serve as an Area Chair for CVPR 2023, NeurIPS 2023, CVPR 2024, ECCV 2024, NeurIPS 2024, ICLR 2025, CVPR 2025.</li>
			<li>I serve as a Senior Program Committee for AAAI 2023, AAAI 2024, and AAAI 2025.</li>
			<li>I serve as an Associate Editor for Pattern Recognition, and a Guest Editor for IEEE TCSVT.</li>
			<li><font color="red">Pinned projects:</font> 1. New innovations: <a href="https://depth-anything.github.io">Depth Anything V1</a> & <a href="https://depth-anything-v2.github.io">V2</a>, <a href="https://arxiv.org/abs/2312.10035">Point Transformer V3</a>, <a href="https://gpt4point.github.io">GPT4Point</a>, <a href="https://arxiv.org/abs/2402.18573">UniMODE</a>, <a href="https://ali-vilab.github.io/AnyDoor-Page">AnyDoor</a>, <a href="https://xavierchen34.github.io/LivePhoto-Page">LivePhoto</a>, <a href="https://xavierchen34.github.io/MimicBrush-Page">MimicBrush</a>, <a href="https://pixelgs.github.io">Pixel-GS</a>; 2. Highly optimized codebase available for 3D scene understanding <a href="https://github.com/Pointcept/Pointcept">Pointcept (PTv1&PTv2&PTv3&MSC&PPT)</a>; 3. Highly optimized codebase available for semantic segmentation <a href="http://github.com/hszhao/semseg">semseg (PSPNet&PSANet)</a>.</li>
		</ul>
	</div> -->

	<!-- Publications -->
	<div class="container" style="padding-top: 20px; font-size: 17px;">
		<h3 id="Publications" style="padding-top: 80px; margin-top: -80px; margin-left: -18px;">My Publications</h3>
		<p><a href="https://scholar.google.com/citations?user=u2yppEsAAAAJ&hl">Google Scholar</a> and <a href="publication.html">Full List</a>.</p>

		<h3 id="Publications" style="padding-top: 80px; margin-top: -80px; margin-left: -18px;"><b>2025</b></h3>
		<hr>
		<div class="row">
			<div class="col-md-3"><div class="badge">ArXiv</div><img class="img-fluid img-rounded" src="teaser/bouba/conformal_2025.png"></div>
			<div class="col-md-9">
			<b><font color="black">Uncertainty Guarantees on Automated Precision Weeding using Conformal Prediction</font></b><br>
			Melki, P., Bombrun, L., <b>Diallo Boubacar</b>, Dias, J., & Da Costa, J. P.<br>
			arXiv preprint arXiv:2501.07185, 2025. 
			[<a href="https://arxiv.org/pdf/2501.07185">Paper</a>]
			<div style="text-align:justify; color: rgb(107, 105, 105);"><i>
				After a detailed presentation of the conformal prediction methodology and the development of a precision spraying pipeline 
				based on a ''conformalized'' neural network and well-defined spraying decision rules, the article evaluates this pipeline 
				on two real-world scenarios: one under in-distribution conditions, the other reflecting a near out-of-distribution setting.
			</i></div>
            </div>
		</div><br>

		<h3 id="Publications" style="padding-top: 80px; margin-top: -80px; margin-left: -18px;"><b>2024</b></h3>
		<hr>
		<div class="row">
			<div class="col-md-3"><div class="badge">CVPR</div><img class="img-fluid img-rounded" src="teaser/bouba/repip.png"></div>
			<div class="col-md-9">
			<b><font color="black">The Penalized Inverse Probability Measure for Conformal Classification</font></b><br>
			Melki, P., Bombrun, L., <b>Diallo Boubacar</b>, Dias, J., & Da Costa, J. P.<br>
			In Proceedings of the Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2024. 
			[<a href="https://openaccess.thecvf.com/content/CVPR2024W/SAIAD/papers/Melki_The_Penalized_Inverse_Probability_Measure_for_Conformal_Classification_CVPRW_2024_paper.pdf">Paper</a>]
			<div style="text-align:justify; color: rgb(107, 105, 105);"><i>
				Through toy examples and empirical results on the task of crop and weed image classification in agricultural robotics, 
				the current work shows how PIP-based conformal classifiers exhibit precisely the desired behavior in comparison with 
				other nonconformity measures and strike a good balance between informativeness and efficiency.
			</i></div>
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ArXiv</div><img class="img-fluid img-rounded" src="teaser/bouba/AL2024.png"></div>
			<div class="col-md-9">
			<b><font color="black">Active learning for efficient annotation in precision agriculture: A use-case on crop-weed semantic segmentation.</font></b><br>
			van Marrewijk, B. M., Dandjinou, C., Rustia, D. J. A., Gonzalez, N. F., <b>Diallo Boubacar</b> & Blok, P. M..<br>
			(<b>ICCV 2023</b>) - <b>ArXiv</b> preprint arXiv:2404.02580, 2024. [<a href="https://arxiv.org/pdf/2404.02580">Paper</a>]
			<div style="text-align:justify; color: rgb(107, 105, 105);"><i>
				This study addresses this research gap by conducting a comparative study of three active learning-based acquisition functions: 
				Bayesian Active Learning by Disagreement (BALD), stochastic-based BALD (PowerBALD), and Random. 
				The acquisition functions were tested on two agricultural datasets: Sugarbeet and Corn-Weed, both containing 
				three semantic classes: background, crop and weed.
			</i></div>
            </div>
		</div><br>

		<h3 id="Publications" style="padding-top: 80px; margin-top: -80px; margin-left: -18px;"><b>2023</b></h3>
		<hr>
		<div class="row">
			<div class="col-md-3"><div class="badge">AgriTech</div><img class="img-fluid img-rounded" src="teaser/bouba/impact.png"></div>
			<div class="col-md-9">
			<b><font color="black">Impact of the Environmental Conditions on the Robustness of a Weed Detection Model</font></b><br>
			Nicolas Franco Gonzalez, Paul Melki, <b>Boubacar Diallo</b>, Hugo Antoine, Estelle Millet.<br>
			AgriTech Day (<b>SIMA</b>), 2023. 
			[<a href="https://agritechday.com/wp-content/uploads/2022/12/AGRITECHDAY_CONFERENCE_PROCEEDINGS_2022.pdf">Paper</a>]
			<div style="text-align:justify; color: rgb(107, 105, 105);"><i>
				The main contribution of this article is the proposal of a methodology for analyzing the impact of this variability 
				on weed detection quality. We focus only on four intrinsic and external factors that contribute to the variability 
				of conditions (which we call meta-variables) : the vegetation development stage (using the BBCH scale), the soil 
				characteristics, namely its color and humidity level, and the condition of the sky at the moment of image acquisition (sunny or overcast).
			</i></div>
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ICCV</div><img class="img-fluid img-rounded" src="teaser/bouba/aps2023_0.png"></div>
			<div class="col-md-9">
			<b><font color="black">Group-Conditional Conformal Prediction via Quantile Regression Calibration for Crop and Weed Classification</font></b><br>
			Paul Melki, Lionel Bombrun, <b>Boubacar Diallo</b>, J√©r√¥me Dias, Jean-Pierre Da Costa<br>
			Proceedings of the IEEE/CVF International Conference on Computer Vision (<b>ICCV</b>), 2023. 
			[<a href="https://openaccess.thecvf.com/content/ICCV2023W/CVPPA/papers/Melki_Group-Conditional_Conformal_Prediction_via_Quantile_Regression_Calibration_for_Crop_and_ICCVW_2023_paper.pdf">Paper</a>]
			<div style="text-align:justify; color: rgb(107, 105, 105);"><i>
				This article presents the conformal prediction framework that provides valid statistical guarantees on 
				the predictive performance of any black box prediction machine, with almost no assumptions, 
				applied to the problem of deep visual classification of weeds and crops in real-world conditions.
			</i></div>
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">ICCV Poster</div><img class="img-fluid img-rounded" src="teaser/bouba/AL2024.png"></div>
			<div class="col-md-9">
			<b><font color="black">Active learning for efficient annotation in crop-weed semantic segmentation.</font></b><br>
			Blok, P. M., van Marrewijk, B. M., <b>Boubacar Diallo</b>, Dandjinou, C., Gonzalez, N. F., Melki, P., ... & Dias, J.<br>
			Proceedings of the IEEE/CVF International Conference on Computer Vision (<b>ICCV</b>), 2023. 
			[<a href="https://cvppa2023.github.io/assets/38.pdf">Paper</a>]
			<div style="text-align:justify; color: rgb(107, 105, 105);"><i>
				In agriculture, datasets tend to contain more redundant images and imbalanced classes. 
				Therefore, in this research the added value of active learning was tested on a Corn-Weed dataset. 
				Three acquisition functions were compared: BALD, PowerBALD and Random. 
				Both BALD and PowerBALD outperformed Random sampling even when 90.9% of the pixels belonged to the background class.
			</i></div>
            </div>
		</div><br>

		<h3 id="Publications" style="padding-top: 80px; margin-top: -80px; margin-left: -18px;"><b>2022</b></h3>
		<hr>
		<div class="row">
			<div class="col-md-3"><div class="badge">CEA: Elsevier</div><img class="img-fluid img-rounded" src="teaser/bouba/maskal.png"></div>
			<div class="col-md-9">
			<b><font color="black">
				Active learning with MaskAL reduces annotation effort for training Mask R-CNN on a broccoli dataset with visually similar classes
			</font></b><br>
			Pieter M Blok, Gert Kootstra, Hakim Elchaoui Elghor, <b>Boubacar Diallo</b>, Frits K van Evert, E. van Henten.<br>
			Computers and Electronics in Agriculture,  Elsevier (<b>CEA: Elsevier</b>), 2022. 
			[<a href="https://www.sciencedirect.com/science/article/pii/S0168169922002344">Paper</a>]
			<div style="text-align:justify; color: rgb(107, 105, 105);"><i>
				The goal of our work was to reduce the number of annotated images needed to train a CNN while maintaining its performance. 
				We use an active learning method capable of automatically selecting difficult-to-classify images and based on the Mask R-CNN 
				instance segmentation algorithm and named this method MaskAL.
			</i></div>
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">GRETSI</div><img class="img-fluid img-rounded" src="teaser/bouba/explo_2022.png"></div>
			<div class="col-md-9">
			<b><font color="black">Analyse exploratoire de m√©triques en segmentation d'images: application en proxid√©tection</font></b><br>
			Melki, P., Bombrun, L., Millet, E., <b>Boubacar Diallo</b>, Elghor, H. E., & da Costa, J. P.<br>
			In Groupe d'Etudes du Traitement du Signal et des Images (<b>GRETSI</b>), 2022.
			[<a href="https://www.mdpi.com/2072-4292/14/4/996">Paper</a>]
			<div style="text-align:justify; color: rgb(107, 105, 105);"><i>
				Working in an agricultural context, specifically on the problem of the automatic detection of plants in proximal 
				sensing images, we studied twelve evaluation metrics that we used to evaluate three image segmentation models 
				recently presented in the literature. After a unified presentation of these metrics, we carried out an exploratory 
				analysis of their relationships using a correlation analysis, a clustering of variables, and two factorial analyses 
				(namely principal component analysis and multiple factorial analysis). .
			</i></div>
            </div>
		</div><br>

		<h3 id="Publications" style="padding-top: 80px; margin-top: -80px; margin-left: -18px;"><b>2020</b></h3>
		<hr>
		<div class="row">
			<div class="col-md-3"><div class="badge">Ph.D Report</div>
			<img class="img-fluid img-rounded" src="teaser/bouba/thesis.png"></div>
			<div class="col-md-9">
			<b><font color="black">Mesure de l'int√©grit√© d'une image : des mod√®les physiques aux mod√®les d'apprentissage profond</font></b><br>
			<b>Boubacar Diallo</b>, HAL science ouverte, December 2020. [<a href="https://hal.science/tel-03212896v1">Paper</a>]<br>
			<div style="text-align:justify; color: rgb(107, 105, 105);"><i>
				In this thesis, we focus on several of these image forensic challenges including camera
				model identification and image tampering detection. After reviewing the state of the
				art in the field, we propose a first data-driven method for identifying camera models.
				We use deep learning techniques based on convolutional neural networks (CNNs)
				and develop a learning strategy considering the quality of the input data versus the
				applied transformation. A family of CNN networks has been designed to learn the
				characteristics of the camera model directly from a collection of images undergoing
				the same transformations as those commonly used on the Internet.
			</i></div>
            </div>
		</div><hr>

		<div class="row">
			<div class="col-md-3"><div class="badge">FSI</div><img class="img-fluid img-rounded" src="teaser/bouba/robustness.png"></div>
			<div class="col-md-9">
			<b><font color="black">Robust forgery detection for compressed images using CNN supervision</font></b><br>
			<b>Boubacar Diallo</b>, Thierry Urruty, Pascal Bourdon, Christine Fernandez-Maloigne.<br>
			Forensic Science International: Reports (<b>FSI: Reports</b>), 2020.
			[<a href="https://www.sciencedirect.com/science/article/pii/S266591072030061X?ref=pdf_download&fr=RR-2&rr=8e200e4d499c6ed2">Paper</a>]
			<div style="text-align:justify; color: rgb(107, 105, 105);"><i>
				This article presents a framework improving robustness for image forgery detection. 
				The most important step of our framework is to take into account the image quality corresponding 
				to the chosen application. Therefore, we relied on a camera identification model based on convolutional 
				neural networks.
			</i></div>
            </div>
		</div><br>

		<h3 id="Publications" style="padding-top: 80px; margin-top: -80px; margin-left: -18px;"><b>2019</b></h3>
		<hr>
		<div class="row">
			<div class="col-md-3"><div class="badge">MMM</div><img class="img-fluid img-rounded" src="teaser/bouba/mmm2019.png"></div>
			<div class="col-md-9">
			<b><font color="black">Improving robustness of image tampering detection for compression</font></b><br>
			<b>Boubacar Diallo</b>, Thierry Urruty, Pascal Bourdon, Christine Fernandez-Maloigne.<br>
			MultiMedia Modeling: 25th International Conference (<b>MMM</b>), 2019. 
			[<a href="https://hal.science/hal-02401565/file/Paper_MMM2019.pdf">Paper</a>]
			<div style="text-align:justify; color: rgb(107, 105, 105);"><i>
				Improving the robustness of tampered image detection algorithms is essential for compression manipulation 
				as it is the most common type of post-processing on the Internet.
            </i></div>
			</div>
		</div><hr>

	</div><br>
	<script>var scroll = new SmoothScroll('a[href*="#"]', {speed: 1000});</script>
</body>
</html>